{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8884d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber loss\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true-y_pred\n",
    "    is_small_error = tf.abs(error)<1\n",
    "    squared_loss = tf.square(error)/2\n",
    "    linear_loss = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "# where( a, b, c)は aのTrueに対応する要素でb\n",
    "#                   aのFalseに対応する要素でcを代入して新しいtensor.arrayを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5a6dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model with huber_loss\n",
    "model.compile(loss=huber_fn, optimizer=\"nadam\")\n",
    "model.fit(X_train, y_train, [...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d03685c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to save and load models containing custom components\n",
    "# load custom_objects = {\"xx\":...}\n",
    "\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "#                                  custom_objects={\"huber_fn\": huber_fn})\n",
    "\n",
    "# change the threshold\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true-y_pred\n",
    "        is_small_error = tf.abs(error)< threshold\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "# threshold is not saved\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "#                                  custom=objects={\"huber_fn\":create_huber(2.0)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac74f861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.5, 1.5],\n",
       "       [0.5, 0.5]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_huber(threshold=2.0)(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "         error = y_true-y_pred\n",
    "        is_small_error = tf.abs(error)< self.threshold\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss = self.threshold *tf.abs(error)-self.threshold**2/2\n",
    "    def get_config(self):\n",
    "        base_config = super.get_config()\n",
    "        return {**base_config, \"threshold\":self.threshold}\n",
    "        \n",
    "# explanation of this code \n",
    "# P497-498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "512cdf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Alice\n",
      "age: 25\n",
      "city: New York\n"
     ]
    }
   ],
   "source": [
    "# example of kwargs\n",
    "# **kwargs →assuming dictionary typed input\n",
    "def print_person_info(**kwargs):\n",
    "    for key, value in kwargs.items():\n",
    "        print(key + \": \" + str(value))\n",
    "\n",
    "print_person_info(name=\"Alice\", age=25, city=\"New York\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b6bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n",
    "model = keras.models.load_models(\"my_model_with_a_custom_loss_class.h5\",\n",
    "                                  custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24401649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a custom activation function\n",
    "#  a custom glorot_inirializer\n",
    "#  a custom l1 regularizer\n",
    "#  a custom constraint that ensures weights to be all positive\n",
    "def my_softplus(z): # return value is tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z)+1.0)\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. /(shape[0]+shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights< 0., tf.zeros_like(weights), weights)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2b3d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(30, activation=my_softplus,\n",
    "                           kernel_initializer= my_glorot_initializer,\n",
    "                           kernel_regularizer = my_l1_regularizer,\n",
    "                           kernel_constraint = my_positive_weights)\n",
    "# if hyperparameters have to be saved, subclass should be used\n",
    "\n",
    "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}\n",
    "# __call__ メソッドはclassを二回呼び出すと自動的に呼び出さる\n",
    "# to use is \n",
    "# kernel_regularizer=MyL1Regularizer(factor=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0511f24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Adder object at 0x000001DB68CFA550>\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# example:\n",
    "class Adder:\n",
    "    def __init__(self, num):\n",
    "        self.num = num\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.num + x\n",
    "\n",
    "# クラスのインスタンスを関数として呼び出す\n",
    "adder = Adder(5)\n",
    "result = adder(3)\n",
    "num=3\n",
    "print(adder)\n",
    "print(Adder(num)(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce404040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(21, shape=(), dtype=int32)\n",
      "tf.Tensor([5 7 9], shape=(3,), dtype=int32)\n",
      "tf.Tensor([ 6 15], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# examples for reduce_sum\n",
    "x = tf.constant([[1, 2, 3],\n",
    "                 [4, 5, 6]])\n",
    "\n",
    "# 全ての要素の合計\n",
    "total_sum = tf.reduce_sum(x)\n",
    "\n",
    "# 列方向（軸=0）に要素の合計を計算\n",
    "column_sum = tf.reduce_sum(x, axis=0)\n",
    "\n",
    "# 行方向（軸=1）に要素の合計を計算\n",
    "row_sum = tf.reduce_sum(x, axis=1)\n",
    "\n",
    "# 結果の表示\n",
    "print(total_sum)\n",
    "print(column_sum)\n",
    "print(row_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c9c45028",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [39]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Custom Metrics\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# metrics：評価関数、微分不可能でもよく、モデルを評価する際だけ使う\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# custom losses can be directly used as metrics\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# example:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnadam\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[create_huber(\u001b[38;5;241m2.0\u001b[39m)])\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# for binary classification, the accuracy should be operated by:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m predicision \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Custom Metrics\n",
    "# metrics：評価関数、微分不可能でもよく、モデルを評価する際だけ使う\n",
    "# custom losses can be directly used as metrics\n",
    "# example:\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2bc056b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for binary classification, the accuracy should be operated by:\n",
    "precision = keras.metrics.Precision()\n",
    "precision([0,1,1,1,0,1,0,1],[1,1,0,1,0,1,0,1])\n",
    "# ([labels],[predictions])\n",
    "# 4/5 = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8ef38cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
    "# 4/(5+3) = 0.5\n",
    "# batches are accumulated in precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfe2467f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.result() # current value of the metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da18a86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision.variables # tracking the number of true and false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5af9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states() # reset both variables to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1cc4d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass from keras.metrics.Metric class\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true-y_pred\n",
    "        is_small_error = tf.abs(error)< threshold\n",
    "        squared_loss = tf.square(error)/2\n",
    "        linear_loss = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn\n",
    "# add_weight is from keras.metrics.Metric\n",
    "# \n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "        self.huber_fn = create_huber(theshold)\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count=self.add_weight(\"count\",initializer=\"zeros\")\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total/self.count\n",
    "    def get_config(self):\n",
    "        base_config = super.get_config()\n",
    "        return {**base_config, \"threshold\":self.threshold}\n",
    "# the sum of all huber losses(total) and the number of instances seen so far(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e7649933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Layers\n",
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "394f93f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.085537 20.085537 20.085537]\n"
     ]
    }
   ],
   "source": [
    "# example of tf.exp()\n",
    "uu = tf.Variable([3,3,3], dtype=tf.float32)\n",
    "result = tf.exp(uu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a2f314ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this exponential_layer could be used a an layer of exponential activation function\n",
    "# equivalent to\n",
    "# activation=tf.exp, activation = keras.activations.exponential, or activation=\"exponential\"\n",
    "\n",
    "# to build a custom stateful layer with weights, use subclassing\n",
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units=units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel=self.add_weight(name=\"kernel\",\n",
    "                                    shape=[batch_input_shape[-1], self.units],\n",
    "                                    initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(name=\"bias\",\n",
    "                                    shape=[self.units],\n",
    "                                    initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel +self.bias)\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1]+[self.units])\n",
    "    def get_config(self):\n",
    "        base_config = super.get_config()\n",
    "        return {**base_config, \"units\":self.units,\n",
    "                 \"activation\":keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3620289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build multiple inputs layer\n",
    "# the call() method should return the list of outputs\n",
    "# compute_output_shape() should return the list of batch output shapes\n",
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return [X1 + X2, X1 * X2, X1 / X2]\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        b1, b2 = batch_input_shape\n",
    "        return [b1, b1, b1] # should probably handle broadcasting rules\n",
    "# this layer takes two inputs and returns three outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b9c14fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to provide different behaviors during training and during testing,\n",
    "# adding a training argument to the call() methos and use this argument to decide\n",
    "# what to do\n",
    "class MyGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X),stddev = self.stddev)\n",
    "            return X +noise\n",
    "        else:\n",
    "            return X\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f69f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "26c5a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom models\n",
    "# P508\n",
    "# build a ResidualBlockLayer\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\" )for _ in range(n_layers)]\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)  # this calculate the output (when inputs=Z )\n",
    "        return inputs+Z  # add outputs of inputs and inputs together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dc987d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1+3):    #最初のResidualBlockで3回繰り返す構造を作りたい\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)\n",
    "# implement get_config() method to make the model to be able to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1857a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom model with a custom reconstruction loss:\n",
    "class ReconstructingRegressor(keras.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super.__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                       kernel_initializer = \"lecun_normal\") for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "    def build(self, batch_input_shape):\n",
    "        n_inputs=batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z) # ここでbuildメソッドが起動されて,batch_input_shape = Zで入る\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        return self.out(Z)\n",
    "# interpretations \n",
    "# P511-512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d103e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "baf4883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3*w1**2 +2*w1*w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d32d0001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.000003007075065"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#　近似的手法では\n",
    "w1, w2=5,3\n",
    "eps=1e-6 #微小量の変化\n",
    "(f(w1+eps,w2)-f(w1,w2))/eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d146b7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.000000003174137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(f(w1,w2+eps)-f(w1,w2))/eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "430e6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tensorflow's autodiff\n",
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z=f(w1,w2)\n",
    "gradients = tape.gradient(z, [w1,w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a3d2b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88c3392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(30.0, shape=(), dtype=float32) None\n"
     ]
    }
   ],
   "source": [
    "# tape.gradients get delected after one autodiff\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z=f(w1,w2)\n",
    "dzdw1 = tape.gradient(z,w1)\n",
    "dzdw2 = tape.gradient(z,w2)\n",
    "del tape #persistent後、free resources with del tape\n",
    "print(dzdw1, dzdw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "080ed7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "# tape will only track operations involving variables, \n",
    "# with c1 ,c2 = tf.constant(5.), tf.constant(3.)\n",
    "# returns [None, None]\n",
    "c1 ,c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z=f(c1,c2)\n",
    "gradients = tape.gradient(z, [c1,c2])\n",
    "print(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "37aa3c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make none-variables able to do autodiff\n",
    "# set tape.watch(xx)\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1,c2)\n",
    "gradients = tape.gradient(z,[c1,c2])\n",
    "gradients\n",
    "# 実応用例：inputsを基にしたloss計算(inputs はconstantだからtapeの必要がある)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ba45a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second order derivatives\n",
    "# P 515 and Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec38226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to stop gradients from backpropagating through some part of the neural network\n",
    "def f(w1, w2):\n",
    "    return 3*w1**2 +tf.stop_gradient(2*w1*w2)\n",
    "with tf.GradientTape() as tape:\n",
    "    z=f(w1,w2)\n",
    "gradients = tape.gradient(z, [w1,w2])\n",
    "gradients# returns [tensor=30, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9edfcaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)\n"
     ]
    }
   ],
   "source": [
    "# numerical issues like my_softplus() function for large inputs\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)\n",
    "# this is due to a floating-point precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3781cc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(30.0, shape=(), dtype=float32) None\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2) # works now!\n",
    "del tape\n",
    "print(dz_dw1, dz_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82417b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1,), dtype=float32, numpy=array([nan], dtype=float32)>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([100.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7c5b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dixed softplus function to solve the numerical problem\n",
    "@tf.custom_gradient\n",
    "def my_better_softplus(z):\n",
    "    exp = tf.exp(z)\n",
    "    def my_softplus_gradients(grad):\n",
    "        return grad / (1 + 1 / exp)\n",
    "    return tf.math.log(exp + 1), my_softplus_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3cec504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1,), dtype=float32, numpy=array([inf], dtype=float32)>,\n",
       " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([1.], dtype=float32)>])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.Variable([1000.])\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_better_softplus(x)\n",
    "\n",
    "z, tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "795b7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Loops\n",
    "# example: implement two different optimizers for Wide & Deep neural network\n",
    "# first, build a simple model\n",
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04a9eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that randomly sample a batch of instances from the training set\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8b64167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that will display the training status\n",
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics= \" - \".join([\"{}:{:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss]+(metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total)+metrics, end=end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d99803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "47d36285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "11610/11610 - mean:1.4502 - mean_absolute_error:0.5950\n",
      "Epoch 2/5\n",
      "11610/11610 - mean:0.6594 - mean_absolute_error:0.5270\n",
      "Epoch 3/5\n",
      "11610/11610 - mean:0.6385 - mean_absolute_error:0.5215\n",
      "Epoch 4/5\n",
      "11610/11610 - mean:0.6509 - mean_absolute_error:0.5261\n",
      "Epoch 5/5\n",
      "11610/11610 - mean:0.6554 - mean_absolute_error:0.5266\n"
     ]
    }
   ],
   "source": [
    "# define some hyperparameters and choose the optimizer, the loss function, and the metrics\n",
    "n_epochs=5\n",
    "batch_size = 32\n",
    "n_steps=len(X_train)//batch_size\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]\n",
    "\n",
    "# build the custom loop ( equivalent to .fit)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)#要素ごと加算する\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()\n",
    "# tqdm method to build the custom loop\n",
    "# Github In[204]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2424b46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow functions and Graphs\n",
    "def cube(x):\n",
    "    return x**3\n",
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd2add84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0a2e0d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.eager.polymorphic_function.polymorphic_function.Function at 0x272d4a922e0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube = tf.function(cube)\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ed76b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a04dbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9c4cb452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function as a decoration\n",
    "\n",
    "@tf.function\n",
    "def tf_cube(x):\n",
    "    return x**3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ed7e619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bf0b7ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function tf_cube at 0x00000272D59ADB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ed289d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function tf_cube at 0x00000272D59ADB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1000, 8000])>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(tf.constant([10,20])) # if tf_cube([10, 20]), error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b71d8a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more details and tips P522-523\n",
    "# TF function is most of the time faster especially if it performs complex computations\n",
    "concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d2a35b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.framework.func_graph.FuncGraph at 0x272d6d8aee0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_function.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4ead75cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=1000>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_cube(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "222d454c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sum_squares' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [82]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tf\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mto_code(\u001b[43msum_squares\u001b[49m\u001b[38;5;241m.\u001b[39mpython_function)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sum_squares' is not defined"
     ]
    }
   ],
   "source": [
    "tf.autograph.to_code(sum_squares.python_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c782a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
