{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba0051b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different initionalizers to solve vanishing gradients problems\n",
    "# P 432-433\n",
    "\n",
    "# keras ReLU uses Glorot initialization with a uniform distribution\n",
    "# setting keras.layers.Dense(..., kernel_initializer=\"he_normal\")\n",
    "# means using He initialization\n",
    "\n",
    "\n",
    "\n",
    "# he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg',distribution='uniform')\n",
    "# keras.layers.Dense(10, activation=\"sigmoid\",\n",
    "# kernel_initializer=he_avg_init)\n",
    "\n",
    "# this sets He initialization based on fanavg instead of fanin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc093c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nonsaturating Activation Functions\n",
    "# ReLU doesnt saturating ,but neurons may die and continue to output only 0\n",
    "# this happends when the weighted sum of the weights are positive\n",
    "# ReLU's gradient is zero at this case so the backpropagation doesnt work\n",
    "\n",
    "# leaky ReLU P434\n",
    "# parametric leaky ReLU P434\n",
    "# randomized leaky ReLU P434 (outperform ReLU on large image datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbce97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exponential linear unit( ELU)\n",
    "# outperforms all ReLU\n",
    "# P 435-436\n",
    "# the darkback is that it is slower to compute\n",
    "\n",
    "# Scaled ELU ( SELU) P437\n",
    "# how to use in keras P438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2843061c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch normalization\n",
    "# 1.avoids standardscaler\n",
    "# 2.avoids exploding gradients and vanishing gradients\n",
    "\n",
    "# however, not useful in RNNs or other complex types of neural networks\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\",use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias = False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")\n",
    "    \n",
    "])\n",
    "# this codes adds BatchNormalization before activation functions\n",
    "# P444\n",
    "# better test if this works better on current dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b2127d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Gradient Clipping\u001b[39;00m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(clipvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Gradient Clipping\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "# this method has a lot problems, not recommended\n",
    "# clipnorm=1.0 doesnt change its orientation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46178af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning with Keras\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.0\n",
    "X_test = X_test / 255.0\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]\n",
    "\n",
    "# X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "# X_train_B: a much smaller training set of just the first 200 images of sandals or shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d2e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfdcd8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc097e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5054468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.6016 - accuracy: 0.8127 - val_loss: 0.3895 - val_accuracy: 0.8655\n",
      "Epoch 2/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.3592 - accuracy: 0.8764 - val_loss: 0.3273 - val_accuracy: 0.8881\n",
      "Epoch 3/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.3216 - accuracy: 0.8879 - val_loss: 0.3000 - val_accuracy: 0.8999\n",
      "Epoch 4/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.3013 - accuracy: 0.8951 - val_loss: 0.2859 - val_accuracy: 0.9021\n",
      "Epoch 5/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2872 - accuracy: 0.9010 - val_loss: 0.2767 - val_accuracy: 0.9083\n",
      "Epoch 6/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2769 - accuracy: 0.9041 - val_loss: 0.2705 - val_accuracy: 0.9061\n",
      "Epoch 7/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2681 - accuracy: 0.9075 - val_loss: 0.2668 - val_accuracy: 0.9071\n",
      "Epoch 8/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2608 - accuracy: 0.9110 - val_loss: 0.2608 - val_accuracy: 0.9131\n",
      "Epoch 9/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2553 - accuracy: 0.9127 - val_loss: 0.2586 - val_accuracy: 0.9106\n",
      "Epoch 10/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2500 - accuracy: 0.9145 - val_loss: 0.2524 - val_accuracy: 0.9136\n",
      "Epoch 11/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2452 - accuracy: 0.9166 - val_loss: 0.2470 - val_accuracy: 0.9163\n",
      "Epoch 12/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2409 - accuracy: 0.9181 - val_loss: 0.2462 - val_accuracy: 0.9145\n",
      "Epoch 13/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2374 - accuracy: 0.9189 - val_loss: 0.2435 - val_accuracy: 0.9126\n",
      "Epoch 14/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2338 - accuracy: 0.9207 - val_loss: 0.2390 - val_accuracy: 0.9170\n",
      "Epoch 15/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2308 - accuracy: 0.9217 - val_loss: 0.2423 - val_accuracy: 0.9148\n",
      "Epoch 16/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2276 - accuracy: 0.9226 - val_loss: 0.2369 - val_accuracy: 0.9165\n",
      "Epoch 17/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2248 - accuracy: 0.9233 - val_loss: 0.2394 - val_accuracy: 0.9168\n",
      "Epoch 18/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2217 - accuracy: 0.9249 - val_loss: 0.2481 - val_accuracy: 0.9141\n",
      "Epoch 19/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2195 - accuracy: 0.9251 - val_loss: 0.2331 - val_accuracy: 0.9175\n",
      "Epoch 20/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2172 - accuracy: 0.9263 - val_loss: 0.2327 - val_accuracy: 0.9175\n",
      "Epoch 21/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2144 - accuracy: 0.9272 - val_loss: 0.2385 - val_accuracy: 0.9163\n",
      "Epoch 22/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2127 - accuracy: 0.9272 - val_loss: 0.2278 - val_accuracy: 0.9203\n",
      "Epoch 23/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2106 - accuracy: 0.9279 - val_loss: 0.2272 - val_accuracy: 0.9190\n",
      "Epoch 24/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2083 - accuracy: 0.9290 - val_loss: 0.2272 - val_accuracy: 0.9200\n",
      "Epoch 25/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2063 - accuracy: 0.9292 - val_loss: 0.2274 - val_accuracy: 0.9185\n",
      "Epoch 26/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.2044 - accuracy: 0.9305 - val_loss: 0.2233 - val_accuracy: 0.9215\n",
      "Epoch 27/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2027 - accuracy: 0.9300 - val_loss: 0.2234 - val_accuracy: 0.9208\n",
      "Epoch 28/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.2008 - accuracy: 0.9312 - val_loss: 0.2209 - val_accuracy: 0.9215\n",
      "Epoch 29/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1993 - accuracy: 0.9316 - val_loss: 0.2252 - val_accuracy: 0.9200\n",
      "Epoch 30/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1973 - accuracy: 0.9326 - val_loss: 0.2196 - val_accuracy: 0.9240\n",
      "Epoch 31/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1962 - accuracy: 0.9328 - val_loss: 0.2213 - val_accuracy: 0.9230\n",
      "Epoch 32/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1939 - accuracy: 0.9344 - val_loss: 0.2238 - val_accuracy: 0.9223\n",
      "Epoch 33/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1927 - accuracy: 0.9346 - val_loss: 0.2179 - val_accuracy: 0.9230\n",
      "Epoch 34/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1911 - accuracy: 0.9348 - val_loss: 0.2200 - val_accuracy: 0.9245\n",
      "Epoch 35/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1896 - accuracy: 0.9352 - val_loss: 0.2202 - val_accuracy: 0.9230\n",
      "Epoch 36/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1882 - accuracy: 0.9358 - val_loss: 0.2150 - val_accuracy: 0.9243\n",
      "Epoch 37/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1868 - accuracy: 0.9362 - val_loss: 0.2190 - val_accuracy: 0.9228\n",
      "Epoch 38/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1855 - accuracy: 0.9368 - val_loss: 0.2147 - val_accuracy: 0.9253\n",
      "Epoch 39/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1842 - accuracy: 0.9366 - val_loss: 0.2249 - val_accuracy: 0.9200\n",
      "Epoch 40/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1824 - accuracy: 0.9370 - val_loss: 0.2118 - val_accuracy: 0.9270\n",
      "Epoch 41/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1807 - accuracy: 0.9381 - val_loss: 0.2173 - val_accuracy: 0.9260\n",
      "Epoch 42/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1797 - accuracy: 0.9383 - val_loss: 0.2168 - val_accuracy: 0.9253\n",
      "Epoch 43/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1784 - accuracy: 0.9388 - val_loss: 0.2145 - val_accuracy: 0.9245\n",
      "Epoch 44/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1775 - accuracy: 0.9400 - val_loss: 0.2144 - val_accuracy: 0.9253\n",
      "Epoch 45/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1760 - accuracy: 0.9393 - val_loss: 0.2114 - val_accuracy: 0.9245\n",
      "Epoch 46/50\n",
      "1375/1375 [==============================] - 2s 2ms/step - loss: 0.1749 - accuracy: 0.9408 - val_loss: 0.2169 - val_accuracy: 0.9250\n",
      "Epoch 47/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1742 - accuracy: 0.9408 - val_loss: 0.2094 - val_accuracy: 0.9285\n",
      "Epoch 48/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1723 - accuracy: 0.9415 - val_loss: 0.2146 - val_accuracy: 0.9258\n",
      "Epoch 49/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1711 - accuracy: 0.9413 - val_loss: 0.2083 - val_accuracy: 0.9268\n",
      "Epoch 50/50\n",
      "1375/1375 [==============================] - 2s 1ms/step - loss: 0.1704 - accuracy: 0.9416 - val_loss: 0.2112 - val_accuracy: 0.9285\n"
     ]
    }
   ],
   "source": [
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "histroy = model_A.fit(X_train_A, y_train_A, epochs=50,\n",
    "                      validation_data=(X_valid_A, y_valid_A),\n",
    "                      callbacks=early_stopping_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ee935dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb513d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a model B to see how model_B_on_A improves training speed and accuracy\n",
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10b1c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f3740e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 0.7702 - accuracy: 0.4450 - val_loss: 0.6771 - val_accuracy: 0.6329\n",
      "Epoch 2/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.6088 - accuracy: 0.6850 - val_loss: 0.5722 - val_accuracy: 0.7221\n",
      "Epoch 3/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.5115 - accuracy: 0.7600 - val_loss: 0.4982 - val_accuracy: 0.7840\n",
      "Epoch 4/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4435 - accuracy: 0.8300 - val_loss: 0.4422 - val_accuracy: 0.8286\n",
      "Epoch 5/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3913 - accuracy: 0.8550 - val_loss: 0.3952 - val_accuracy: 0.8570\n",
      "Epoch 6/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3471 - accuracy: 0.9000 - val_loss: 0.3582 - val_accuracy: 0.8813\n",
      "Epoch 7/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3133 - accuracy: 0.9200 - val_loss: 0.3282 - val_accuracy: 0.8986\n",
      "Epoch 8/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2847 - accuracy: 0.9350 - val_loss: 0.3023 - val_accuracy: 0.9148\n",
      "Epoch 9/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2607 - accuracy: 0.9450 - val_loss: 0.2783 - val_accuracy: 0.9290\n",
      "Epoch 10/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2394 - accuracy: 0.9650 - val_loss: 0.2605 - val_accuracy: 0.9402\n",
      "Epoch 11/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2223 - accuracy: 0.9750 - val_loss: 0.2452 - val_accuracy: 0.9422\n",
      "Epoch 12/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2068 - accuracy: 0.9750 - val_loss: 0.2303 - val_accuracy: 0.9473\n",
      "Epoch 13/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1928 - accuracy: 0.9750 - val_loss: 0.2179 - val_accuracy: 0.9533\n",
      "Epoch 14/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1809 - accuracy: 0.9750 - val_loss: 0.2073 - val_accuracy: 0.9554\n",
      "Epoch 15/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1703 - accuracy: 0.9800 - val_loss: 0.1962 - val_accuracy: 0.9604\n",
      "Epoch 16/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1606 - accuracy: 0.9850 - val_loss: 0.1872 - val_accuracy: 0.9615\n",
      "Epoch 17/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1520 - accuracy: 0.9850 - val_loss: 0.1789 - val_accuracy: 0.9655\n",
      "Epoch 18/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1442 - accuracy: 0.9850 - val_loss: 0.1713 - val_accuracy: 0.9675\n",
      "Epoch 19/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1368 - accuracy: 0.9850 - val_loss: 0.1651 - val_accuracy: 0.9686\n",
      "Epoch 20/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1304 - accuracy: 0.9850 - val_loss: 0.1590 - val_accuracy: 0.9686\n",
      "Epoch 21/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1243 - accuracy: 0.9900 - val_loss: 0.1531 - val_accuracy: 0.9706\n",
      "Epoch 22/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1186 - accuracy: 0.9900 - val_loss: 0.1484 - val_accuracy: 0.9706\n",
      "Epoch 23/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9900 - val_loss: 0.1438 - val_accuracy: 0.9716\n",
      "Epoch 24/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1096 - accuracy: 0.9900 - val_loss: 0.1395 - val_accuracy: 0.9716\n",
      "Epoch 25/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1055 - accuracy: 0.9900 - val_loss: 0.1359 - val_accuracy: 0.9716\n",
      "Epoch 26/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1014 - accuracy: 0.9900 - val_loss: 0.1318 - val_accuracy: 0.9736\n",
      "Epoch 27/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0978 - accuracy: 0.9900 - val_loss: 0.1285 - val_accuracy: 0.9757\n",
      "Epoch 28/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0947 - accuracy: 0.9900 - val_loss: 0.1254 - val_accuracy: 0.9757\n",
      "Epoch 29/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0917 - accuracy: 0.9950 - val_loss: 0.1225 - val_accuracy: 0.9757\n",
      "Epoch 30/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0888 - accuracy: 0.9950 - val_loss: 0.1194 - val_accuracy: 0.9757\n",
      "Epoch 31/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0862 - accuracy: 0.9950 - val_loss: 0.1164 - val_accuracy: 0.9757\n",
      "Epoch 32/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0835 - accuracy: 0.9950 - val_loss: 0.1139 - val_accuracy: 0.9757\n",
      "Epoch 33/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0809 - accuracy: 0.9950 - val_loss: 0.1113 - val_accuracy: 0.9757\n",
      "Epoch 34/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0786 - accuracy: 0.9950 - val_loss: 0.1092 - val_accuracy: 0.9767\n",
      "Epoch 35/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0764 - accuracy: 0.9950 - val_loss: 0.1072 - val_accuracy: 0.9757\n",
      "Epoch 36/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0744 - accuracy: 0.9950 - val_loss: 0.1048 - val_accuracy: 0.9767\n",
      "Epoch 37/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0721 - accuracy: 0.9950 - val_loss: 0.1028 - val_accuracy: 0.9777\n",
      "Epoch 38/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0702 - accuracy: 0.9950 - val_loss: 0.1009 - val_accuracy: 0.9787\n",
      "Epoch 39/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0685 - accuracy: 0.9950 - val_loss: 0.0990 - val_accuracy: 0.9817\n",
      "Epoch 40/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0668 - accuracy: 0.9950 - val_loss: 0.0969 - val_accuracy: 0.9817\n",
      "Epoch 41/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0650 - accuracy: 0.9950 - val_loss: 0.0955 - val_accuracy: 0.9807\n",
      "Epoch 42/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0634 - accuracy: 0.9950 - val_loss: 0.0942 - val_accuracy: 0.9828\n",
      "Epoch 43/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0621 - accuracy: 0.9950 - val_loss: 0.0923 - val_accuracy: 0.9828\n",
      "Epoch 44/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9950 - val_loss: 0.0916 - val_accuracy: 0.9828\n",
      "Epoch 45/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0591 - accuracy: 0.9950 - val_loss: 0.0900 - val_accuracy: 0.9828\n",
      "Epoch 46/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 0.9950 - val_loss: 0.0886 - val_accuracy: 0.9828\n",
      "Epoch 47/50\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0563 - accuracy: 0.9950 - val_loss: 0.0873 - val_accuracy: 0.9828\n",
      "Epoch 48/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0552 - accuracy: 0.9950 - val_loss: 0.0859 - val_accuracy: 0.9838\n",
      "Epoch 49/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9950 - val_loss: 0.0847 - val_accuracy: 0.9838\n",
      "Epoch 50/50\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.0528 - accuracy: 0.9950 - val_loss: 0.0836 - val_accuracy: 0.9838\n"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=5)\n",
    "histroy = model_B.fit(X_train_B, y_train_B, epochs=50,\n",
    "                      validation_data=(X_valid_B, y_valid_B),\n",
    "                      callbacks=early_stopping_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5da020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f67f5580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 8)                 408       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,158\n",
      "Trainable params: 276,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53afae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x1ba95c893a0>,\n",
       " <keras.layers.core.dense.Dense at 0x1ba957d4a90>,\n",
       " <keras.layers.core.dense.Dense at 0x1ba95c89c40>,\n",
       " <keras.layers.core.dense.Dense at 0x1ba95c8e880>,\n",
       " <keras.layers.core.dense.Dense at 0x1ba95c76df0>,\n",
       " <keras.layers.core.dense.Dense at 0x1ba95c973d0>,\n",
       " <keras.layers.core.dense.Dense at 0x1ba95c8e700>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_A.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a76fc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, reuse all layers\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "# however, drop the last layer because it has a different output shape\n",
    "# this codes make model_B_on_A contains part of A, clone another model_A can avoid\n",
    "# model_A to be changed while training model_B_on_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b894b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b0530bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_4 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B_on_A.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efe8fc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.3395 - accuracy: 0.8700 - val_loss: 0.2620 - val_accuracy: 0.9168\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2170 - accuracy: 0.9550 - val_loss: 0.1923 - val_accuracy: 0.9574\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.1618 - accuracy: 0.9700 - val_loss: 0.1526 - val_accuracy: 0.9675\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1291 - accuracy: 0.9750 - val_loss: 0.1280 - val_accuracy: 0.9746\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.1086 - accuracy: 0.9750 - val_loss: 0.1116 - val_accuracy: 0.9787\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0945 - accuracy: 0.9800 - val_loss: 0.0993 - val_accuracy: 0.9838\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0839 - accuracy: 0.9800 - val_loss: 0.0904 - val_accuracy: 0.9848\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0761 - accuracy: 0.9850 - val_loss: 0.0830 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0696 - accuracy: 0.9850 - val_loss: 0.0768 - val_accuracy: 0.9899\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0642 - accuracy: 0.9850 - val_loss: 0.0720 - val_accuracy: 0.9919\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0599 - accuracy: 0.9850 - val_loss: 0.0673 - val_accuracy: 0.9929\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0557 - accuracy: 0.9850 - val_loss: 0.0637 - val_accuracy: 0.9929\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0525 - accuracy: 0.9850 - val_loss: 0.0606 - val_accuracy: 0.9929\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0497 - accuracy: 0.9850 - val_loss: 0.0577 - val_accuracy: 0.9929\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0470 - accuracy: 0.9900 - val_loss: 0.0553 - val_accuracy: 0.9939\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.0448 - accuracy: 0.9900 - val_loss: 0.0532 - val_accuracy: 0.9939\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", \n",
    "                     optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
    "histroy = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8c26b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 0.0428 - accuracy: 0.9900 - val_loss: 0.0509 - val_accuracy: 0.9939\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0404 - accuracy: 0.9900 - val_loss: 0.0491 - val_accuracy: 0.9939\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0385 - accuracy: 0.9900 - val_loss: 0.0470 - val_accuracy: 0.9939\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0362 - accuracy: 0.9900 - val_loss: 0.0456 - val_accuracy: 0.9939\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0347 - accuracy: 0.9900 - val_loss: 0.0444 - val_accuracy: 0.9939\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0334 - accuracy: 0.9900 - val_loss: 0.0433 - val_accuracy: 0.9939\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0322 - accuracy: 0.9900 - val_loss: 0.0424 - val_accuracy: 0.9939\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0312 - accuracy: 0.9900 - val_loss: 0.0414 - val_accuracy: 0.9939\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0302 - accuracy: 0.9950 - val_loss: 0.0406 - val_accuracy: 0.9939\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0292 - accuracy: 0.9950 - val_loss: 0.0398 - val_accuracy: 0.9939\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0284 - accuracy: 0.9950 - val_loss: 0.0389 - val_accuracy: 0.9939\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0275 - accuracy: 0.9950 - val_loss: 0.0381 - val_accuracy: 0.9939\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0266 - accuracy: 0.9950 - val_loss: 0.0375 - val_accuracy: 0.9939\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0259 - accuracy: 0.9950 - val_loss: 0.0368 - val_accuracy: 0.9939\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0250 - accuracy: 0.9950 - val_loss: 0.0361 - val_accuracy: 0.9939\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.0240 - accuracy: 0.9950 - val_loss: 0.0355 - val_accuracy: 0.9939\n"
     ]
    }
   ],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(learning_rate=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                 validation_data=(X_valid_B, y_valid_B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bdb328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 877us/step - loss: 0.0328 - accuracy: 0.9955\n"
     ]
    }
   ],
   "source": [
    "check_B_on_A = model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eff0476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 869us/step - loss: 0.0751 - accuracy: 0.9830\n"
     ]
    }
   ],
   "source": [
    "check_B = model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f6ecd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01250004768371582"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_B_on_A[1] - check_B[1]\n",
    "# the model's test accuracy is higher with fewer epochs of training\n",
    "# however, transfer learning does not always work well\n",
    "# but performs quite well on small dense networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95fec10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsupervised Pretraining\n",
    "# P451- 453"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "313bce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faster optimizers\n",
    "# P 454-\n",
    "# 1.Momentum Optimization\n",
    "#   adds a momentum vector, speeding traditional Gradient Descent by to 10 times\n",
    "#   it helps rolling local optima faster and DNN that dont use Batch Normalization\n",
    "\n",
    "# implementation\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "# just add a momentum, ( 0.9 is a nice default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e15a9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Nosterov Accelerated Gradient\n",
    "#   this is always faster than vanilla momentum optimization\n",
    "\n",
    "#implementation\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
    "                                 nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11889888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.AdaGrad\n",
    "#   this optimizer points directly to the global optiminum instead of going\n",
    "#    down to the direction of the local optimum\n",
    "#    AdaGrad stops too fast before reaching the global optimum though it is fast\n",
    "#    avoid using Adagrad when training neural network is a wise choice\n",
    "\n",
    "#    however, it fits well on Linear Regression or other simpler questions\n",
    "#    P458"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b0dc3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.RMSProp\n",
    "#   RMSProp fixes the problem of AdaGrad to never converging to the global optimum\n",
    "#   by accumulating only the gradients from the most recent iterations\n",
    "#   it does so by using exponential decay in the first step\n",
    "#   P459\n",
    "\n",
    "#implementation\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "# rho = 0.9 works well most of the time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5351e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Adam ( Adaptive moment estimation)\n",
    "#   combines the ideas of momentum optimization and RMSProp\n",
    "#   it keeps track of an exponentially decaying average of past gradients\n",
    "#   P459-460\n",
    "\n",
    "#implementation\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.001, beta_1 =0.9, beta_2 = 0.999)\n",
    "# the default learning_rate is good enough because Adam is an adaptive learning rate\n",
    "# algorithm (dont have to choose the best learning_rate, instead, setting it\n",
    "# to default does most of the work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db30427d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.AdaMax\n",
    "#   more stable than Adam but depends on datasets, Adam is better most of the time\n",
    "#   than AdaMax\n",
    "#   P461"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cee66ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.Nadam\n",
    "#   Nadam is Adam + Nesterov trick, which often converges slightly faster than Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d7d84b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a comparison of these optimizers\n",
    "# P463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89779d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cac0e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduling\n",
    "# instead of using an optimal but constant learning rate,\n",
    "# starting with a low learning rate, increase and then drop again may be a better\n",
    "# solution and this is  call learning shcedules\n",
    "# 1. Power Scheduling \n",
    "# P464\n",
    "\n",
    "# implementation\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, decay=1e-4)\n",
    "# setting the decay implements power scheduling, controlling\n",
    "# how fast the learning_rate decays\n",
    "# decay is the inverse of s ( the numbers of steps it takes to divide the learning\n",
    "# rate by one more unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "544f7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.Exponential scheduling\n",
    "# P465\n",
    "\n",
    "# implementation\n",
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01*0.1**(epoch/20)\n",
    "# If you do not want to hardcode η and s, you can create a function that returns a\n",
    "# configured function:\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0*0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "exponential_decay_fn =exponential_decay(lr0=0.01, s=20)\n",
    "#この書き方だと\n",
    "# exponential_decay(xx, xx)(xx)\n",
    "#の書き方をする\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "#  histroy = model.fit(X_train_scaled, y_train, [...], callbacks =[lr_scheduler]) \n",
    "# learning_rate will be updated at the beginning of each epoch\n",
    "\n",
    "# .fit sets epochs to start from 1, to avoid hurting the weights with a too big\n",
    "# learning_rate, setting fit( initial_epoch=xxx) manually\n",
    "\n",
    "\n",
    "# e.g.\n",
    "# def fn(x):\n",
    "#     def gn(y):\n",
    "#         return x+y+5\n",
    "#     return gn\n",
    "# fn(5)(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d02eec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.Piecewise constant scheduling\n",
    "# P465\n",
    "\n",
    "#implementation\n",
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch <5:\n",
    "        return 0.01\n",
    "    elif epoch <15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001\n",
    "# and pass it to the callback\n",
    "# to update the learning rate at each iteration rather than at each epoch,\n",
    "# see Github examples In[84] !!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6dfe6cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Performance scheduling\n",
    "# P465\n",
    "\n",
    "# implementation\n",
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor= 0.5, patience=5)\n",
    "# when the val_score doesnt improve for 5 epochs, learning_rate is multiplied by 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d94292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative way to implement learning rate scheduling\n",
    "# example on exponential_decay_fn\n",
    "s = 20*len(X_train) //32 # number of steps in 20 epochs(batch_size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "\n",
    "# with this method, saved models have the learning rate and its schedule as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "49a2d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1cycle scheduling\n",
    "# P465\n",
    "\n",
    "# Github In[96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1e156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c43f6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoiding overfitting Through Regularization\n",
    "# typical ones:\n",
    "# 1. Batch Normalization\n",
    "# 2. early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c54dea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3. l1 and l2 Regularization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# l2 regularization to constrain a neural networks's connection weights\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# l1 regularization when training sparse model\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# implementation\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m layer \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m                            kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_normal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m                            kernel_regularizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.01\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. l1 and l2 Regularization\n",
    "# l2 regularization to constrain a neural networks's connection weights\n",
    "# l1 regularization when training sparse model\n",
    "\n",
    "# implementation\n",
    "layer = keras.layers.Dense(100, activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "# a regularizer is called at each step during training to compute the regularization\n",
    "# loss, which is then added to the final loss\n",
    "\n",
    "# keras.regularizers.l1(x)\n",
    "# keras.regularizers.l1_l2(x,x)\n",
    "# also works\n",
    "\n",
    "from functools import partial\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layer.Flatten(input_shape=[28,28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\",\n",
    "                     kernel_initializer=\"glorot_uniform\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff443f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 4.Dropout\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#   at every training step, every neuron has a probability p of being temporarily\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#   dropped out, meaning it will be entirely ignored during this training step,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# implementation\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m     10\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mFlatten(input_shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m]),\n\u001b[0;32m     11\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     12\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m300\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_normal\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     14\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m100\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melu\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhe_normal\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     15\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m),\n\u001b[0;32m     16\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernal_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglorot_uniform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "# 4.Dropout\n",
    "#   at every training step, every neuron has a probability p of being temporarily\n",
    "#   dropped out, meaning it will be entirely ignored during this training step,\n",
    "#   but it may be active during the next step.\n",
    "# p is called dropout rate ( automatically set to 10%)\n",
    "\n",
    "\n",
    "# implementation\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28,28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\", kernal_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "# increase the dropout rate when observing the model to be overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a67b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Monte Carlo (MC) Dropout\n",
    "\n",
    "#implementation\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "# stack, axis=0 は e.g. np.stack([[0,1,2],[3,4,5]]) = [[1,2,3],[4,5,6]]\n",
    "# this code makes 100 predictions over the test ste, training=True ensures the\n",
    "# Dropout layer is active\n",
    "\n",
    "# to check the predictions made when dropout is activated:\n",
    "np.round(y_probas[:, :1], 2)\n",
    "# take average\n",
    "np.round(y_proba[:1], 2)\n",
    "# standard deviation of the probability estimates\n",
    "y_std = y_probas.std(axis=0) #.stdは標準偏差の計算\n",
    "np.round(y_std[:1], 2)\n",
    "# finally check the accuracy\n",
    "accuracy = np.sum(y_pred == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if special layers like BatchNormalization layers exist,\n",
    "# using MCDropout class instead\n",
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d40312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.Max-Norm Regularization\n",
    "# P477\n",
    "\n",
    "# implementation\n",
    "keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                   kernel_constraint = keras.constraints.max_norm(1.))\n",
    "# after each training iteration, the object returned by max_norm is called and\n",
    "# the layers's weights are replaced by rescaled weights in return\n",
    "# !! when using convolutional layers, set the max_norm constraint's \n",
    "# properly, and this is usually axis=[0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd6809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492caacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A brief summary \n",
    "# P478"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
